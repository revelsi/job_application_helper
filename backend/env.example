# Job Application Helper v1.0.0 - Configuration

# =============================================================================
# LLM API Configuration
# =============================================================================

# OpenAI API Key (Primary LLM - GPT-5-mini with reasoning capabilities)
OPENAI_API_KEY=your_openai_api_key_here

# Mistral AI API Key (Alternative LLM - Mistral Small/Medium models)
MISTRAL_API_KEY=your_mistral_api_key_here

# Novita API Key (Open-source models - GPT-OSS-20B, Qwen3-32B, GLM-4.5)
NOVITA_API_KEY=your_novita_api_key_here

# Ollama Configuration (Local LLM - no API key required)
# Base URL for Ollama service (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434
# Timeout for Ollama requests (seconds)
OLLAMA_TIMEOUT=60

# Default LLM Provider (optional - auto-selects based on availability)
# Options: openai, mistral, novita, ollama
DEFAULT_LLM_PROVIDER=

# =============================================================================
# Embeddings Configuration
# =============================================================================
# Sentence Transformers model used for semantic routing in QueryAnalyzer
# Recommended default: sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
# Optional cache dir for embeddings model (use Docker volume for persistence)
# EMBEDDING_CACHE_DIR=/home/appuser/.cache/torch/sentence_transformers

# Provider Selection:
# - If API keys are set above, those providers will be automatically available
# - If no keys are set, configure providers through the web UI
# - Only one provider can be active at a time

# =============================================================================
# Application Settings
# =============================================================================

# Environment (production, development)
ENVIRONMENT=production

# Application Port
PORT=8000

# Log Level (INFO, WARNING, ERROR, DEBUG)
LOG_LEVEL=INFO

# =============================================================================
# Data Storage Configuration
# =============================================================================

# Local data directory path
DATA_DIR=./data

# Documents storage path
DOCUMENTS_PATH=./data/documents

# Cache directory path
CACHE_PATH=./data/cache

# =============================================================================
# Security Settings
# =============================================================================

# Enable/disable encryption for sensitive data (true/false)
# Default: true (encryption enabled for security)
ENABLE_ENCRYPTION=true

# Encryption key for sensitive data (auto-generated on first run if not provided)
# To generate manually: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
ENCRYPTION_KEY=your_encryption_key_here

# Maximum file upload size (in MB)
MAX_UPLOAD_SIZE_MB=10

# =============================================================================
# Document Processing Configuration
# =============================================================================

# Maximum context length for LLM (total across all documents)
# GPT-5-mini supports 128K tokens (~500K characters), other models vary
# Default: 100,000 characters (~25K tokens) provides good balance across all models
MAX_CONTEXT_LENGTH=100000

# Per-document context limits (characters)
# These control how much content is included from each document type
# With GPT-5-mini's large context window, we can include full documents

# Maximum content from candidate documents (CV, cover letters, certificates, etc.)
# Default allows for very detailed CVs and multiple documents
MAX_CANDIDATE_DOC_LENGTH=50000

# Maximum content from job description documents
# Default allows for comprehensive job postings with detailed requirements
MAX_JOB_DOC_LENGTH=30000

# Maximum content from company information documents  
# Default allows for detailed company profiles, values, and culture info
MAX_COMPANY_DOC_LENGTH=20000

# Maximum tokens for chat responses (fallback for older configurations)
# NOTE: Token limits are now automatically calculated based on the specific model used
# GPT-5-mini supports up to 16K output tokens, other models vary
# This setting is only used if dynamic token management fails
CHAT_MAX_TOKENS=16000

# =============================================================================
# API Rate Limiting
# =============================================================================

# Rate limiting (requests per minute) - applies to all LLM providers
API_RATE_LIMIT=60

# Python version: 3.9+ required 